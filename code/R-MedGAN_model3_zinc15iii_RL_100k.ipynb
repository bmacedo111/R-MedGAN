{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedGAN\n",
    "\n",
    "Model 3 | Subset ZINC15-III | Reinforcement Learning | 100k quinolines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import gc\n",
    "import io\n",
    "import json\n",
    "import csv\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from rdkit.Chem import rdmolops\n",
    "from rdkit.Chem.rdmolops import AddHs\n",
    "from rdkit.Chem.rdmolops import GetMolFrags\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import AllChem, Draw, Descriptors\n",
    "from rdkit.Chem import AtomValenceException\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem import rdmolfiles\n",
    "from rdkit.Chem.Draw import IPythonConsole, MolsToGridImage\n",
    "from rdkit.Chem.Fingerprints import FingerprintMols\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from rdkit.DataStructs.cDataStructs import TanimotoSimilarity\n",
    "import rdkit.RDLogger as rdl\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "from keras import initializers\n",
    "from keras.layers import BatchNormalization\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import base64\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from PIL import Image\n",
    "import gzip\n",
    "import pickle\n",
    "import psutil\n",
    "import pygraphviz as pgv\n",
    "import time\n",
    "import shutil\n",
    "RDLogger.DisableLog(\"rdApp.*\")\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "logger = rdl.logger()\n",
    "logger.setLevel(rdl.ERROR)\n",
    "logger.setLevel(rdl.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing: Filtering and Sampling of Quinoline Molecules\n",
    "\n",
    "The following section is dedicated to loading and processing quinoline molecule data from the ZINC15 dataset. Initially, the script checks whether a pre-filtered dataset (non_duplicate_filtered_quinolines_zinc15_50atoms.csv) exists. If it doesn't, the original dataset (non_duplicate_quinolines_zinc15.csv) is loaded and molecules are filtered based on specific criteria:\n",
    "\n",
    "Only molecules containing the atoms - Carbon (C), Nitrogen (N), Oxygen (O), Hydrogen (H), Fluorine (F), Sulfur (S), and Chlorine (Cl) are retained.\n",
    "    \n",
    "Molecules having up to a maximum of 50 atoms, including hydrogen, are considered.\n",
    "\n",
    "Post filtering, duplicates are removed, and the processed data is saved for future use. If the filtered dataset already exists, it is directly loaded.\n",
    "\n",
    "To ensure consistent results and manageable data sizes, a subsample of 100,000 molecules is randomly chosen from this dataset. This subsample is then saved for subsequent analyses. Lastly, as a quick verification, one molecule is printed to visualize its structure and compute its count of heavy atoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load filtered csv or save a new one\n",
    "csv_path = \"../data/non_duplicate_filtered_quinolines_zinc15_50atoms.csv\"\n",
    "filtered_csv_path = \"../data/non_duplicate_filtered_quinolines_zinc15_50atoms.csv\"\n",
    "\n",
    "allowed_atoms = {'C', 'N', 'O', 'H', 'F', 'S', 'Cl'}\n",
    "max_atoms = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def has_allowed_atoms(mol, allowed_atoms):\n",
    "    for atom in mol.GetAtoms():\n",
    "        if atom.GetSymbol() not in allowed_atoms:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def total_atoms(mol):\n",
    "    return sum(atom.GetTotalNumHs() + 1 for atom in mol.GetAtoms())\n",
    "\n",
    "def is_valid_smiles(smiles):\n",
    "    molecule = Chem.MolFromSmiles(smiles)\n",
    "    molecule = Chem.AddHs(molecule)\n",
    "    if (molecule is not None\n",
    "        and '*' not in smiles\n",
    "        and has_allowed_atoms(molecule, allowed_atoms)\n",
    "        and total_atoms(molecule) <= max_atoms):\n",
    "        return smiles\n",
    "    return None\n",
    "\n",
    "def get_cpu_usage():\n",
    "    return psutil.cpu_percent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If filtered CSV doesn't exist, read the original CSV, apply the filter, and save the filtered data\n",
    "if not os.path.isfile(filtered_csv_path):\n",
    "    # Read CSV file\n",
    "    data = pd.read_csv(csv_path, usecols=lambda col: col != 'index', header=None)\n",
    "\n",
    "    valid_smiles = []\n",
    "    for smiles in tqdm(data[0]):\n",
    "        try:\n",
    "            result = is_valid_smiles(smiles)\n",
    "            if result is not None:\n",
    "                valid_smiles.append(result)\n",
    "        except Exception as e:\n",
    "            print(\"Error processing item:\", e)\n",
    "\n",
    "    # Remove duplicates\n",
    "    valid_smiles = list(set(valid_smiles))\n",
    "\n",
    "    # Create DataFrame with valid SMILES\n",
    "    data = pd.DataFrame(valid_smiles, columns=['smiles'])\n",
    "    \n",
    "    # Save the filtered data to a CSV file\n",
    "    data.to_csv(filtered_csv_path, index=False)\n",
    "\n",
    "# If filtered CSV exists, just load it\n",
    "else:\n",
    "    data = pd.read_csv(filtered_csv_path)\n",
    "\n",
    "data = data.sample(n=100000, random_state=1, replace=False)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the subsample to a CSV file\n",
    "data.to_csv(\"../data/non_duplicate_filtered_quinolines_zinc15_50atoms.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a sample from the dataset\n",
    "\n",
    "smiles_test = data['smiles'][100]\n",
    "print(\"SMILES:\", smiles_test)\n",
    "molecule = Chem.MolFromSmiles(smiles_test)\n",
    "molecule = Chem.AddHs(molecule)\n",
    "print(\"Num heavy atoms:\", molecule.GetNumHeavyAtoms())\n",
    "molecule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert SMILEs to Graph representation\n",
    "\n",
    "This code provides a suite of tools for representing molecules as graphs and vice versa. The main components are:\n",
    "\n",
    "- Mappings: Dictionaries (atom_mapping, bond_mapping, charge_mapping) are used to translate atom and bond types between their string names and numerical indices.\n",
    "Conversion Functions:\n",
    "\n",
    "- smiles_to_graph(): Converts SMILES strings into graph matrices.\n",
    "\n",
    "- graph_to_molecule(): Reconstructs molecules from their graph matrices.\n",
    "\n",
    "- graph_to_networkx(): Transforms graph matrices into NetworkX graph objects for visualization or algorithmic analysis.\n",
    "\n",
    "- Visualization: plot_graph() visually represents a molecule using the NetworkX graph, with atoms and bonds color-coded.\n",
    "\n",
    "- Data Preprocessing: Molecular data is chunked and processed in parallel to convert batches of SMILES strings into graph representations, which are then saved as compressed files for efficiency.\n",
    "\n",
    "If molecules are already converted to graphs and saved, will be loaded from compressed files.\n",
    "\n",
    "Overall, these tools facilitate the transition between chemical molecular structures and their graph representations, offering a foundation for graph-based molecular analyses or neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_mapping = {\n",
    "    \"C\": 0,\n",
    "    \"N\": 1,\n",
    "    \"O\": 2,\n",
    "    \"H\": 3,\n",
    "    \"F\": 4,\n",
    "    \"S\": 5,\n",
    "    \"Cl\": 6,\n",
    "}\n",
    "\n",
    "bond_mapping = {\n",
    "    \"SINGLE\": 0,\n",
    "    0: Chem.BondType.SINGLE,\n",
    "    \"DOUBLE\": 1,\n",
    "    1: Chem.BondType.DOUBLE,\n",
    "    \"TRIPLE\": 2,\n",
    "    2: Chem.BondType.TRIPLE,\n",
    "    \"AROMATIC\": 3,\n",
    "    3: Chem.BondType.AROMATIC,\n",
    "}\n",
    "\n",
    "charge_mapping = {\n",
    "    -1: 7,\n",
    "    1: 8,\n",
    "}\n",
    "\n",
    "NUM_ATOMS = 50\n",
    "ATOM_DIM = 11  # 7 atom types + 2 for chirality and 2 for charge\n",
    "BOND_DIM = 4 + 1\n",
    "LATENT_DIM = 256\n",
    "\n",
    "# Convert to bidirectional mapping for simplicity\n",
    "atom_mapping.update({v: k for k, v in atom_mapping.items()})\n",
    "\n",
    "def graph_to_molecule(graph):\n",
    "    adjacency, features = graph\n",
    "    molecule = Chem.RWMol()\n",
    "\n",
    "    # Filter out 'padding' atoms and bonds\n",
    "    keep_idx = np.where(\n",
    "        (np.sum(features[:, :ATOM_DIM], axis=1) != 0)  # not a 'padding' atom\n",
    "        & (np.sum(adjacency[:-1], axis=(0, 1)) != 0)  # connected to something\n",
    "    )[0]\n",
    "    \n",
    "    features = features[keep_idx]\n",
    "    adjacency = adjacency[:, keep_idx, :][:, :, keep_idx]\n",
    "\n",
    "    num_neighbors = np.sum(adjacency, axis=(0, 2))\n",
    "\n",
    "    for i in range(len(keep_idx)):\n",
    "        atom_type_idx = np.argmax(features[i, 0:7])\n",
    "        atom_type = atom_mapping[atom_type_idx]\n",
    "        atom = Chem.Atom(atom_type)\n",
    "\n",
    "        # Handle charge\n",
    "        charge_features = features[i, 7:9]  \n",
    "        atom_charge = None\n",
    "        for charge_value, charge_idx in charge_mapping.items():\n",
    "            if charge_features[charge_idx - 7] == 1.0:  # Adjusting index for subset\n",
    "                atom_charge = charge_value\n",
    "                break\n",
    "\n",
    "        if atom_charge is not None:\n",
    "            atom.SetFormalCharge(atom_charge)\n",
    "\n",
    "        # Handle chirality with neighbor check\n",
    "        if num_neighbors[i] >= 3:  # only consider atoms with at least 3 neighbors\n",
    "            if features[i, 9] == 1:\n",
    "                atom.SetChiralTag(Chem.ChiralType.CHI_TETRAHEDRAL_CCW)\n",
    "            elif features[i, 10] == 1:\n",
    "                atom.SetChiralTag(Chem.ChiralType.CHI_TETRAHEDRAL_CW)\n",
    "\n",
    "        _ = molecule.AddAtom(atom)\n",
    "\n",
    "    (bonds_ij, atoms_i, atoms_j) = np.where(np.triu(adjacency) == 1)\n",
    "    for (bond_ij, atom_i, atom_j) in zip(bonds_ij, atoms_i, atoms_j):\n",
    "        if atom_i == atom_j or bond_ij == BOND_DIM - 1:\n",
    "            continue\n",
    "        bond_type = bond_mapping[bond_ij]\n",
    "        molecule.AddBond(int(atom_i), int(atom_j), bond_type)\n",
    "\n",
    "    # Sanitize the molecule\n",
    "    try:\n",
    "        # Attempt to sanitize the molecule\n",
    "        flag = Chem.SanitizeMol(molecule, catchErrors=True)\n",
    "        \n",
    "        # Check the flag returned by the sanitization\n",
    "        if flag != Chem.SanitizeFlags.SANITIZE_NONE:\n",
    "            return None  \n",
    "\n",
    "    except AtomValenceException as e:\n",
    "        print(f\"AtomValenceException during molecule sanitization: {e}\")\n",
    "        return None\n",
    "    except ValueError as e:\n",
    "        if \"Violation occurred\" in str(e) and \"MolFileStereochem.cpp\" in str(e):\n",
    "            print(f\"Chirality error during molecule sanitization: {e}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error during molecule sanitization: {e}\")\n",
    "        return None\n",
    "\n",
    "    return molecule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sample is set for a subsample of 100,000 graphs already converted from molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = \"../data/data_zinc15_subset-iii/\"\n",
    "chunk_size = 100000\n",
    "\n",
    "num_chunks = len(data) // chunk_size\n",
    "#print(num_chunks)\n",
    "\n",
    "def process_chunk(i):\n",
    "    adjacency_tensor_file = os.path.join(save_folder, f'adjacency_tensor_{i}.npz')\n",
    "    feature_tensor_file = os.path.join(save_folder, f'feature_tensor_{i}.npz')\n",
    "\n",
    "    # If tensors exist, load them (decompressed)\n",
    "    adjacency_tensor = np.load(adjacency_tensor_file)['arr_0']\n",
    "    feature_tensor = np.load(feature_tensor_file)['arr_0']\n",
    "    \n",
    "    print(f\"Loaded tensors from chunk {i}\")\n",
    "\n",
    "    return adjacency_tensor, feature_tensor\n",
    "\n",
    "adjacency_tensors = []\n",
    "feature_tensors = []\n",
    "\n",
    "# Process each chunk individually after loading\n",
    "for i in tqdm(range(num_chunks), desc='Loading data chunks'):\n",
    "    adjacency_tensor, feature_tensor = process_chunk(i)\n",
    "    adjacency_tensors.append(adjacency_tensor)\n",
    "    feature_tensors.append(feature_tensor)\n",
    "\n",
    "adjacency_tensor = np.concatenate(adjacency_tensors)\n",
    "feature_tensor = np.concatenate(feature_tensors)\n",
    "\n",
    "print(\"adjacency_tensor.shape =\", adjacency_tensor.shape)\n",
    "print(\"feature_tensor.shape =\", feature_tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN R-GCN implementation\n",
    "\n",
    "This code provides a detailed walkthrough of the implementation of WGAN R-GCN using the Keras library. We break down the components into two main sections: the Graph Generator and the Graph Discriminator.\n",
    "\n",
    "The Graph Generator function, GraphGenerator, is responsible for creating a graph representation of a molecule given latent space inputs.\n",
    "\n",
    "The Graph Discriminator function, GraphDiscriminator, assesses the \"realness\" of a molecule graph. It uses graph convolution layers to process the adjacency and feature matrices, and provides a scalar output representing the authenticity of the input graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GraphGenerator(\n",
    "    dense_units, dropout_rate, latent_dim, adjacency_shape, feature_shape, l1_value=0.1\n",
    "):\n",
    "    \n",
    "    # placeholder for the AutoEncoder\n",
    "    \n",
    "    z = keras.layers.Input(shape=(LATENT_DIM,))\n",
    "    # Propagate through one or more densely connected layers\n",
    "    x = z\n",
    "    for units in dense_units:\n",
    "        #x = keras.layers.BatchNormalization()(x) #    \n",
    "        x = keras.layers.Dense(units, activation=\"tanh\")(x) #, kernel_regularizer=regularizers.l1(l1_value)\n",
    "        x = keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Map outputs of previous layer (x) to [continuous] adjacency tensors (x_adjacency)\n",
    "    x_adjacency = keras.layers.Dense(tf.math.reduce_prod(adjacency_shape))(x)\n",
    "    x_adjacency = keras.layers.Reshape(adjacency_shape)(x_adjacency)\n",
    "    # Symmetrify tensors in the last two dimensions\n",
    "    x_adjacency = (x_adjacency + tf.transpose(x_adjacency, (0, 1, 3, 2))) / 2\n",
    "    x_adjacency = keras.layers.Softmax(axis=1)(x_adjacency)\n",
    "\n",
    "    # Map outputs of previous layer (x) to [continuous] feature tensors (x_features)\n",
    "    x_features = keras.layers.Dense(tf.math.reduce_prod(feature_shape))(x)\n",
    "    x_features = keras.layers.Reshape(feature_shape)(x_features)\n",
    "    x_features = keras.layers.Softmax(axis=2)(x_features)\n",
    "\n",
    "    return keras.Model(inputs=z, outputs=[x_adjacency, x_features], name=\"Generator\")\n",
    "\n",
    "generator = GraphGenerator(\n",
    "    dense_units=[128, 256, 512, 1024, 2048, 4096],\n",
    "    dropout_rate=0.10,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n",
    "    feature_shape=(NUM_ATOMS, ATOM_DIM),\n",
    ")\n",
    "generator.summary()\n",
    "\n",
    "# Save the model summary to a text file\n",
    "with open('generator_summary_model3_zinc15_iii.txt', 'w') as f:\n",
    "    generator.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "# Save the model structure as an image\n",
    "plot_model(generator, to_file='generator_model3_zinc15_iii.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalGraphConvLayer(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units=128,  # 128\n",
    "        activation=\"relu\",\n",
    "        use_bias=False,\n",
    "        kernel_initializer=\"glorot_uniform\",    \n",
    "        bias_initializer=\"zeros\",\n",
    "        kernel_regularizer=None,\n",
    "        #kernel_regularizer=regularizers.l1(0.01),    #None,\n",
    "        bias_regularizer=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = keras.initializers.get(bias_initializer)\n",
    "        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)\n",
    "        self.bias_regularizer = keras.regularizers.get(bias_regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        bond_dim = input_shape[0][1]\n",
    "        atom_dim = input_shape[1][2]\n",
    "\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(bond_dim, atom_dim, self.units),\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            trainable=True,\n",
    "            name=\"W\",\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(\n",
    "                shape=(bond_dim, 1, self.units),\n",
    "                initializer=self.bias_initializer,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                trainable=True,\n",
    "                name=\"b\",\n",
    "                dtype=tf.float32,\n",
    "            )\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        adjacency, features = inputs\n",
    "        # Aggregate information from neighbors\n",
    "        x = tf.matmul(adjacency, features[:, None, :, :])\n",
    "        # Apply linear transformation\n",
    "        x = tf.matmul(x, self.kernel)\n",
    "        if self.use_bias:\n",
    "            x += self.bias\n",
    "        # Reduce bond types dim\n",
    "        x_reduced = tf.reduce_sum(x, axis=1)\n",
    "        # Apply non-linear transformation\n",
    "        return self.activation(x_reduced)\n",
    "\n",
    "\n",
    "def GraphDiscriminator(\n",
    "    gconv_units, dense_units, dropout_rate, adjacency_shape, feature_shape, l1_value=0.01\n",
    "):\n",
    "\n",
    "    adjacency = keras.layers.Input(shape=adjacency_shape)\n",
    "    features = keras.layers.Input(shape=feature_shape)\n",
    "\n",
    "    # Propagate through one or more graph convolutional layers\n",
    "    features_transformed = features\n",
    "    for units in gconv_units:\n",
    "        features_transformed = RelationalGraphConvLayer(units)(\n",
    "            [adjacency, features_transformed]\n",
    "        )\n",
    "\n",
    "    # Reduce 2-D representation of molecule to 1-D\n",
    "    x = keras.layers.GlobalAveragePooling1D()(features_transformed)\n",
    "\n",
    "    # Propagate through one or more densely connected layers\n",
    "    for units in dense_units:\n",
    "        #x = keras.layers.Dense(units, activation=\"relu\")(x)\n",
    "        x = keras.layers.Dense(units, activation=\"relu\")(x) #, kernel_regularizer=regularizers.l1(l1_value)\n",
    "        x = keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # For each molecule, output a single scalar value expressing the \"realness\" of the inputted molecule\n",
    "    x_out = keras.layers.Dense(1, dtype=\"float32\")(x)\n",
    "\n",
    "    return keras.Model(inputs=[adjacency, features], outputs=x_out, name='Discriminator')\n",
    "\n",
    "discriminator = GraphDiscriminator(\n",
    "    gconv_units= [512, 512, 512, 512], \n",
    "    dense_units= [4096, 4096],\n",
    "    dropout_rate=0.10,\n",
    "    adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS),\n",
    "    feature_shape=(NUM_ATOMS, ATOM_DIM),\n",
    ")\n",
    "discriminator.summary()\n",
    "\n",
    "# Save the model summary to a text file\n",
    "with open('discriminator_summary_model3_zinc15_iii.txt', 'w') as f:\n",
    "    discriminator.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "# Save the model structure as an image\n",
    "plot_model(discriminator, to_file='discriminator_model3_zinc15_iii.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network for Graphs\n",
    "\n",
    "The GraphWGAN class implements a Generative Adversarial Network (GAN) to generate graphs. This GAN leverages Wasserstein distance with gradient penalty for training stability.\n",
    "\n",
    "The train_step method trains the GAN for one step. This involves training the discriminator to differentiate between real and generated graphs, and training the generator to produce graphs that the discriminator cannot differentiate from real graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphWGAN(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator,\n",
    "        discriminator,\n",
    "        discriminator_steps=1,\n",
    "        generator_steps=1,\n",
    "        gp_weight=10,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.discriminator_steps = discriminator_steps\n",
    "        self.generator_steps = generator_steps\n",
    "        self.gp_weight = gp_weight\n",
    "        self.latent_dim = self.generator.input_shape[-1]\n",
    "        self.epoch = 0\n",
    "        self.batch = 0\n",
    "        self.quinoline_scaffold = Chem.MolFromSmiles(\"n1cccc2ccccc12\")\n",
    "\n",
    "        # We will store our dynamic \"reward\" scalars here\n",
    "        self.value_for_zeros = 1.0\n",
    "        self.value_for_ones = 10.0  # Start at 10 and we will adaptively increase it.\n",
    "\n",
    "        # Initialize metrics for the logs\n",
    "        self.metric_loss_gp = keras.metrics.Mean(name='loss_gp')\n",
    "        self.metric_discriminator_loss = keras.metrics.Mean(name='discriminator_loss')\n",
    "        self.metric_disc_loss = keras.metrics.Mean(name='disc_loss')\n",
    "        self.metric_rewards = keras.metrics.Mean(name='rewards')\n",
    "        self.metric_connectivity_g = keras.metrics.Mean(name='connectivity_g')\n",
    "        self.metric_gen_loss = keras.metrics.Mean(name='gen_loss')\n",
    "        self.metric_critic_g_fake = keras.metrics.Mean(name='critic_g_fake')\n",
    "        self.metric_critic_g_fake_loss = keras.metrics.Mean(name='critic_g_fake_loss')\n",
    "\n",
    "    def compile(self, optimizer_generator, optimizer_discriminator, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.optimizer_generator = optimizer_generator\n",
    "        self.optimizer_discriminator = optimizer_discriminator\n",
    "\n",
    "    # code to generate samples and rewards\n",
    "    def z_to_mol(self, graph, sample_size):\n",
    "        adjacency = tf.argmax(graph[0], axis=1)\n",
    "        adjacency = tf.one_hot(adjacency, depth=BOND_DIM, axis=1)\n",
    "        adjacency = tf.linalg.set_diag(adjacency, tf.zeros(tf.shape(adjacency)[:-1]))\n",
    "        features = tf.argmax(graph[1], axis=2)\n",
    "        features = tf.one_hot(features, depth=ATOM_DIM, axis=2)\n",
    "        molecules = []\n",
    "        for i in range(sample_size):    # for i in range(sample_size):\n",
    "            try:\n",
    "                mol = graph_to_molecule([adjacency[i].numpy(), features[i].numpy()])\n",
    "                if mol:\n",
    "                    molecules.append(mol)\n",
    "                else:\n",
    "                    molecules.append(None)\n",
    "            except AtomValenceException as e:\n",
    "                logging.error(f\"AtomValenceException during molecule generation: {e}\")\n",
    "                molecules.append(None)\n",
    "            except ValueError as e:\n",
    "                if \"Violation occurred\" in str(e) and \"MolFileStereochem.cpp\" in str(e):\n",
    "                    logging.error(f\"Chirality error during molecule generation: {e}\")\n",
    "                    molecules.append(None)\n",
    "                else:\n",
    "                    logging.error(f\"Unexpected ValueError during molecule generation: {e}\")\n",
    "                    molecules.append(None)\n",
    "            except RuntimeError as e:\n",
    "                logging.error(f\"RuntimeError during molecule generation: {e}\")\n",
    "                molecules.append(None)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Unexpected error during molecule generation: {e}\")\n",
    "                molecules.append(None)\n",
    "        return molecules\n",
    "\n",
    "    # code to train\n",
    "    def train_step(self, inputs):\n",
    "        start_time = time.time()\n",
    "        noise_dim = self.generator.input_shape[1]\n",
    "\n",
    "        if isinstance(inputs[0], tuple):\n",
    "            inputs = inputs[0]\n",
    "\n",
    "        real_graphs = inputs\n",
    "        self.batch_size = tf.shape(inputs[0])[0]\n",
    "\n",
    "        def individual_metric_validity(molecule):\n",
    "            if molecule is None or molecule.GetNumAtoms() == 0:\n",
    "                return 0\n",
    "            try:\n",
    "                mol_block = Chem.MolToMolBlock(molecule)\n",
    "                if not all(char == '0' for char in mol_block):\n",
    "                    num_frags = len(rdmolops.GetMolFrags(molecule, asMols=False, sanitizeFrags=False))\n",
    "                    if num_frags == 1:\n",
    "                        return 1\n",
    "                    else:\n",
    "                        return 0\n",
    "            except Exception:\n",
    "                return 0\n",
    "\n",
    "        # Training the discriminator (critic)\n",
    "        for _ in range(self.discriminator_steps):\n",
    "            \n",
    "            with tf.GradientTape() as disc_tape:\n",
    "\n",
    "                z = tf.random.normal([self.batch_size, self.latent_dim])\n",
    "                generated_d_graphs = self.generator(z, training=True)\n",
    "\n",
    "                critic_real = self.discriminator(real_graphs, training=True)\n",
    "                critic_fake = self.discriminator(generated_d_graphs, training=True)\n",
    "\n",
    "                # Calculate gradient penalty\n",
    "                loss_gp = self._gradient_penalty(real_graphs, generated_d_graphs)\n",
    "                discriminator_loss = tf.reduce_mean(critic_fake) - tf.reduce_mean(critic_real)\n",
    "                disc_loss = discriminator_loss + loss_gp * self.gp_weight\n",
    "\n",
    "            gradients_of_discriminator = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
    "            self.optimizer_discriminator.apply_gradients(zip(gradients_of_discriminator, self.discriminator.trainable_variables))\n",
    "\n",
    "            # Update the metric values\n",
    "            self.metric_loss_gp.update_state(loss_gp)    \n",
    "            self.metric_discriminator_loss.update_state(discriminator_loss)    \n",
    "            self.metric_disc_loss.update_state(disc_loss)    \n",
    "\n",
    "        # Training the generator\n",
    "        for _ in range(self.generator_steps):\n",
    "            with tf.GradientTape() as gen_tape:\n",
    "                # Generate fake data here\n",
    "                z = tf.random.normal([self.batch_size, self.latent_dim])\n",
    "                generated_g_graphs = self.generator(z, training=True)\n",
    "\n",
    "                # Convert generated graphs to molecules for reward calculation\n",
    "                generated_g_molecules = self.z_to_mol(graph=generated_g_graphs, sample_size=self.batch_size)\n",
    "                \n",
    "                critic_g_fake = self.discriminator(generated_g_graphs, training=True)\n",
    "                critic_g_fake_tensor = tf.convert_to_tensor(critic_g_fake, dtype=tf.float32)\n",
    "                critic_g_fake_loss = -tf.reduce_mean(critic_g_fake)\n",
    "\n",
    "                # Calculate rewards based on molecule connectivity\n",
    "                rewards = [individual_metric_validity(mol) for mol in generated_g_molecules]\n",
    "                rewards_tensor = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "\n",
    "                connectivity_g_numpy = np.array(rewards)\n",
    "                connectivity_g = np.mean(connectivity_g_numpy)\n",
    "\n",
    "                # We use our dynamic \"value_for_zeros\" and \"value_for_ones\"\n",
    "                scaled_rewards_tensor = tf.where(\n",
    "                    rewards_tensor == 0, \n",
    "                    self.value_for_zeros,   # e.g. 1.0\n",
    "                    self.value_for_ones    # e.g. 10.0, adaptively updated\n",
    "                )\n",
    "\n",
    "                # Reshape scaled rewards for element-wise multiplication with critic scores\n",
    "                adjusted_critic_scores = critic_g_fake_tensor * tf.reshape(scaled_rewards_tensor, (-1, 1))\n",
    "                adjusted_critic_scores_loss = -tf.reduce_mean(adjusted_critic_scores)\n",
    "\n",
    "                lambda_reward = 1.0\n",
    "\n",
    "                # Calculate the generator loss\n",
    "                gen_loss = lambda_reward * adjusted_critic_scores_loss   # -tf.reduce_mean(adjusted_critic_scores)\n",
    "        \n",
    "                # Apply generator gradients\n",
    "                gradients_of_generator = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
    "\n",
    "                # Apply the clipped gradients\n",
    "                self.optimizer_generator.apply_gradients(zip(gradients_of_generator, self.generator.trainable_variables))\n",
    "\n",
    "            # Update the metric values\n",
    "            self.metric_rewards.update_state(rewards)\n",
    "            self.metric_connectivity_g.update_state(connectivity_g)\n",
    "            self.metric_gen_loss.update_state(gen_loss)\n",
    "            self.metric_critic_g_fake.update_state(critic_g_fake)\n",
    "            self.metric_critic_g_fake_loss.update_state(critic_g_fake_loss)\n",
    "\n",
    "        # Increment the epoch counter\n",
    "        self.epoch += 1\n",
    "    \n",
    "        end_time = time.time()\n",
    "        time_per_epoch = end_time - start_time\n",
    "\n",
    "        logs = {m.name: m.result() for m in self.metrics}\n",
    "        logs['time'] = time_per_epoch\n",
    "\n",
    "        return logs\n",
    "\n",
    "    def _gradient_penalty(self, graph_real, graph_generated):\n",
    "        # Unpack graphs\n",
    "        adjacency_real, features_real = graph_real\n",
    "        adjacency_generated, features_generated = graph_generated\n",
    "\n",
    "        # Generate interpolated graphs (adjacency_interp and features_interp)\n",
    "        alpha = tf.random.uniform([self.batch_size])\n",
    "        alpha = tf.reshape(alpha, (self.batch_size, 1, 1, 1))\n",
    "        adjacency_interp = (adjacency_real * alpha) + (1 - alpha) * adjacency_generated\n",
    "        alpha = tf.reshape(alpha, (self.batch_size, 1, 1))\n",
    "        features_interp = (features_real * alpha) + (1 - alpha) * features_generated\n",
    "\n",
    "        # Compute the logits of interpolated graphs\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(adjacency_interp)\n",
    "            tape.watch(features_interp)\n",
    "            logits = self.discriminator(\n",
    "                [adjacency_interp, features_interp], training=True\n",
    "            )\n",
    "\n",
    "        # Compute the gradients with respect to the interpolated graphs\n",
    "        grads = tape.gradient(logits, [adjacency_interp, features_interp])\n",
    "        # Compute the gradient penalty\n",
    "        grads_adjacency_penalty = (1 - tf.norm(grads[0], axis=1)) ** 2\n",
    "        grads_features_penalty = (1 - tf.norm(grads[1], axis=2)) ** 2\n",
    "        return tf.reduce_mean(\n",
    "            tf.reduce_mean(grads_adjacency_penalty, axis=(-2, -1))\n",
    "            + tf.reduce_mean(grads_features_penalty, axis=(-1))\n",
    "        )\n",
    "\n",
    "    def save_model(self, folder_path=\"training_models_model3_zinc15_iii/WGAN\"):\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        self.generator.save(os.path.join(folder_path, \"generator\"))\n",
    "        self.discriminator.save(os.path.join(folder_path, \"discriminator\"))\n",
    "\n",
    "    def load_model(self, folder_path=\"training_models_model3_zinc15_iii/WGAN\"):\n",
    "        self.generator = keras.models.load_model(os.path.join(folder_path, \"generator\"))\n",
    "        self.discriminator = keras.models.load_model(os.path.join(folder_path, \"discriminator\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training metrics\n",
    "\n",
    "This code is a detailed implementation of logging various metrics on Tensorboard related to the progress and performance of a Generative Adversarial Network (GAN) designed to generate molecular structures. It computes metrics such as validity, uniqueness, novelty, quinoline, and scaffold similarity. But also training performance on weights and layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full quinoline dataset\n",
    "quinolines_df = pd.read_csv(\"../data/non_duplicate_filtered_quinolines_zinc15_50atoms.csv\")\n",
    "full_quinolines_smiles = set(quinolines_df['smiles'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, tensorboard_logdir, original_dataset, full_dataset, num_samples):\n",
    "        super().__init__()\n",
    "        self.tensorboard_logdir = tensorboard_logdir\n",
    "        self.writer = tf.summary.create_file_writer(tensorboard_logdir)\n",
    "        self.original_smiles = set(original_dataset)\n",
    "        self.full_quinolines_smiles = set(full_dataset)\n",
    "        self.num_samples = num_samples\n",
    "        self.quinoline_scaffold = Chem.MolFromSmiles(\"n1cccc2ccccc12\")\n",
    "        self.best_connected_validity = 0.0\n",
    "        self.epochs_without_improvement = 0\n",
    "\n",
    "    def mol_sample(self, generator, batch_size):\n",
    "        z = tf.random.normal((batch_size, LATENT_DIM))\n",
    "        graph = generator.predict(z)\n",
    "        adjacency = tf.argmax(graph[0], axis=1)\n",
    "        adjacency = tf.one_hot(adjacency, depth=BOND_DIM, axis=1)\n",
    "        adjacency = tf.linalg.set_diag(adjacency, tf.zeros(tf.shape(adjacency)[:-1]))\n",
    "        features = tf.argmax(graph[1], axis=2)\n",
    "        features = tf.one_hot(features, depth=ATOM_DIM, axis=2)\n",
    "        molecules = []\n",
    "        none_counter = 0\n",
    "        for i in tqdm(range(batch_size), desc=\"Generating molecules\"):\n",
    "            try:\n",
    "                mol = graph_to_molecule([adjacency[i].numpy(), features[i].numpy()])\n",
    "                if mol:\n",
    "                    molecules.append(mol)\n",
    "                else:\n",
    "                    molecules.append(None)\n",
    "                    none_counter += 1\n",
    "            except AtomValenceException as e:\n",
    "                logging.error(f\"AtomValenceException during molecule generation: {e}\")\n",
    "                molecules.append(None)\n",
    "                none_counter += 1\n",
    "            except ValueError as e:\n",
    "                if \"Violation occurred\" in str(e) and \"MolFileStereochem.cpp\" in str(e):\n",
    "                    logging.error(f\"Chirality error during molecule generation: {e}\")\n",
    "                    molecules.append(None)\n",
    "                    none_counter += 1\n",
    "                else:\n",
    "                    logging.error(f\"Unexpected ValueError during molecule generation: {e}\")\n",
    "                    molecules.append(None)\n",
    "                    none_counter += 1\n",
    "            except RuntimeError as e:\n",
    "                logging.error(f\"RuntimeError during molecule generation: {e}\")\n",
    "                molecules.append(None)\n",
    "                none_counter += 1\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Unexpected error during molecule generation: {e}\")\n",
    "                molecules.append(None)\n",
    "                none_counter += 1  # Increment the counter\n",
    "        return molecules, none_counter  # Return the counter\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 0:  # Just trace the graph once, at the beginning\n",
    "            tf.summary.trace_on(graph=True, profiler=True)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        logs = logs or {}\n",
    "        with self.writer.as_default():\n",
    "            # Scalars\n",
    "            tf.summary.scalar('loss_gp', logs.get(\"loss_gp\"), step=epoch)\n",
    "            tf.summary.scalar('discriminator_loss', logs.get(\"discriminator_loss\"), step=epoch)\n",
    "            tf.summary.scalar('disc_loss', logs.get(\"disc_loss\"), step=epoch)\n",
    "            tf.summary.scalar('rewards', logs.get(\"rewards\"), step=epoch)\n",
    "            tf.summary.scalar('connectivity_g', logs.get(\"connectivity_g\"), step=epoch)\n",
    "            tf.summary.scalar('gen_loss', logs.get(\"gen_loss\"), step=epoch)\n",
    "            tf.summary.scalar('critic_g_fake_loss', logs.get(\"critic_g_fake_loss\"), step=epoch)\n",
    "            tf.summary.scalar('critic_g_fake', logs.get(\"critic_g_fake\"), step=epoch)\n",
    "\n",
    "            # Time per epoch\n",
    "            if 'time' in logs:\n",
    "                tf.summary.scalar(\"time_per_epoch\", logs.get(\"time\"), step=epoch)\n",
    "\n",
    "            # Histograms of trainable variables\n",
    "            for var in self.model.trainable_variables:\n",
    "                tf.summary.histogram(var.name, var, step=epoch)\n",
    "\n",
    "            self.writer.flush()\n",
    "\n",
    "        if epoch == 0:  # Write the graph at the end of the first epoch\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.trace_export(name=\"model_trace\", step=epoch, profiler_outdir=self.tensorboard_logdir)\n",
    "\n",
    "        # Evaluate model's generation metrics every epoch or every N epochs\n",
    "        molecules, none_counter = self.mol_sample(self.model.generator, batch_size=100)\n",
    "        none_percentage = none_counter / 100\n",
    "        \n",
    "        valid_molecules = []\n",
    "        for m in molecules:\n",
    "            if m is not None and m.GetNumAtoms() > 0:\n",
    "                try:\n",
    "                    mol_block = Chem.MolToMolBlock(m)\n",
    "                    if not all(char == '0' for char in mol_block):\n",
    "                        valid_molecules.append(m)\n",
    "                except Exception:\n",
    "                    # Silently ignore any errors in block conversion\n",
    "                    pass\n",
    "\n",
    "        connected_valid_molecules = [m for m in valid_molecules if len(GetMolFrags(m)) == 1]\n",
    "        connected_valid_molecules_with_Hs = [AddHs(mol) for mol in connected_valid_molecules]\n",
    "\n",
    "        # Compute Tanimoto similarity to a known scaffold\n",
    "        similarities = []\n",
    "        for m in valid_molecules:\n",
    "            if m.GetNumAtoms() > 0:\n",
    "                try:\n",
    "                    scaffold = MurckoScaffold.GetScaffoldForMol(m)\n",
    "                    similarity = DataStructs.TanimotoSimilarity(\n",
    "                        AllChem.GetMorganFingerprint(self.quinoline_scaffold, 2),\n",
    "                        AllChem.GetMorganFingerprint(scaffold, 2)\n",
    "                    )\n",
    "                    similarities.append(similarity)\n",
    "                except Chem.rdchem.AtomValenceException:\n",
    "                    print(\"Invalid molecule skipped due to AtomValenceException\")\n",
    "\n",
    "        average_similarity = 0\n",
    "        if len(valid_molecules) > 0:\n",
    "            validity = len(valid_molecules) / len(molecules)\n",
    "            if len(connected_valid_molecules) > 0:\n",
    "                connected_validity = len(connected_valid_molecules_with_Hs) / len(molecules)\n",
    "            else:\n",
    "                connected_validity = 0\n",
    "\n",
    "            unique_molecules = list(set(valid_molecules))\n",
    "            if len(unique_molecules) > 0:\n",
    "                uniqueness = len(unique_molecules) / len(valid_molecules)\n",
    "\n",
    "                novel_molecules = [\n",
    "                    Chem.MolToSmiles(mol, isomericSmiles=False, allBondsExplicit=False)\n",
    "                    for mol in unique_molecules\n",
    "                    if Chem.MolToSmiles(mol, isomericSmiles=False, allBondsExplicit=False)\n",
    "                    not in self.original_smiles\n",
    "                ]\n",
    "                novelty = len(novel_molecules) / len(unique_molecules) if len(unique_molecules) > 0 else 0\n",
    "\n",
    "                absolute_novel_molecules = [\n",
    "                    Chem.MolToSmiles(mol, isomericSmiles=False, allBondsExplicit=False)\n",
    "                    for mol in unique_molecules\n",
    "                    if Chem.MolToSmiles(mol, isomericSmiles=False, allBondsExplicit=False)\n",
    "                    not in self.full_quinolines_smiles\n",
    "                ]\n",
    "                absolute_novelty = (\n",
    "                    len(absolute_novel_molecules) / len(unique_molecules)\n",
    "                    if len(unique_molecules) > 0\n",
    "                    else 0\n",
    "                )\n",
    "            else:\n",
    "                uniqueness = 0\n",
    "                novelty = 0\n",
    "                absolute_novelty = 0\n",
    "        else:\n",
    "            validity = 0\n",
    "            novelty = 0\n",
    "            uniqueness = 0\n",
    "            connected_validity = 0\n",
    "            absolute_novelty = 0\n",
    "\n",
    "        if len(valid_molecules) > 0:\n",
    "            quinoline_molecules = [mol for mol in valid_molecules if mol.HasSubstructMatch(self.quinoline_scaffold)]\n",
    "            quinoline_percentage = len(quinoline_molecules) / len(valid_molecules)\n",
    "        else:\n",
    "            quinoline_percentage = 0\n",
    "\n",
    "        if len(similarities) > 0:\n",
    "            average_similarity = sum(similarities) / len(similarities)\n",
    "\n",
    "        if connected_validity > self.best_connected_validity:\n",
    "            self.best_connected_validity = connected_validity\n",
    "            self.epochs_without_improvement = 0\n",
    "        else:\n",
    "            self.epochs_without_improvement += 1\n",
    "            if self.epochs_without_improvement >= 10:\n",
    "                self.model.value_for_ones += 10.0\n",
    "                print(\n",
    "                    f\"=== connected_validity plateaued. \"\n",
    "                    f\"Increasing value_for_ones to {self.model.value_for_ones} ===\"\n",
    "                )\n",
    "                self.epochs_without_improvement = 0\n",
    "\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar(\"validity\", validity, step=epoch)\n",
    "            tf.summary.scalar(\"connected_validity\", connected_validity, step=epoch)\n",
    "            tf.summary.scalar(\"uniqueness\", uniqueness, step=epoch)\n",
    "            tf.summary.scalar(\"novelty\", novelty, step=epoch)\n",
    "            tf.summary.scalar(\"absolute_novelty\", absolute_novelty, step=epoch)\n",
    "            tf.summary.scalar(\"quinoline_percentage\", quinoline_percentage, step=epoch)\n",
    "            tf.summary.scalar(\"average_tanimoto_similarity\", average_similarity, step=epoch)\n",
    "            tf.summary.scalar(\"none_molecule_percentage\", none_percentage, step=epoch)\n",
    "\n",
    "            # Optional: Log the current scaling factor as well\n",
    "            tf.summary.scalar(\"current_value_for_ones\", self.model.value_for_ones, step=epoch)\n",
    "\n",
    "            self.writer.flush()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "In the following code, the WGAN is constructed and trained. The PlotSamplesCallback class is a custom Keras callback to visualize and plot samples at the end of each training epoch. This helps in monitoring the evolution of generated structures over the training process. To ensure continuity and robustness in training, checkpoints are used. A data_generator function is defined to produce training batches from provided tensors, ensuring that the data is fed correctly into the GAN during training. The GAN is then trained using the fit method, with checkpoints, sample plotting, and logging utilities as its callbacks. After training is complete, the model is saved, providing a reusable pre-trained model for future tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotSamplesCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, wgan, num_samples=1):\n",
    "        super().__init__()\n",
    "        self.wgan = wgan\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 1 == 0:\n",
    "            print(\"\\nGenerating and plotting samples at epoch\", epoch)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_mol = MolFromSmiles(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_logdir = \"logs_model3_zinc15_iii/WGAN\"\n",
    "log_counter_G = 0\n",
    "log_counter_D = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the GraphWGAN instance and configure it\n",
    "wgan = GraphWGAN(generator, \n",
    "                 discriminator, \n",
    "                 discriminator_steps=1,\n",
    "                 generator_steps=1,\n",
    "                 gp_weight=10,\n",
    "                 )\n",
    "\n",
    "wgan.compile(\n",
    "    optimizer_generator=keras.optimizers.RMSprop(1e-4),\n",
    "    optimizer_discriminator=keras.optimizers.RMSprop(1e-4)\n",
    ")\n",
    "\n",
    "# Checkpoint configurations\n",
    "checkpoint_dir = 'training_checkpoints_model3_zinc15_iii/WGAN'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    epoch=tf.Variable(0),\n",
    "    generator=wgan.generator,\n",
    "    discriminator=wgan.discriminator,\n",
    "    optimizer_generator=wgan.optimizer_generator,\n",
    "    optimizer_discriminator=wgan.optimizer_discriminator,\n",
    ")\n",
    "\n",
    "def load_latest_checkpoint(checkpoint):\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(f\"Loading checkpoint from {latest_checkpoint}\")\n",
    "        checkpoint.restore(latest_checkpoint)\n",
    "        last_epoch = int(checkpoint.epoch.numpy())\n",
    "    else:\n",
    "        print(\"No checkpoint found. Training from scratch.\")\n",
    "        last_epoch = 0\n",
    "    return last_epoch\n",
    "\n",
    "# Load the latest checkpoint and get the starting epoch\n",
    "starting_epoch = load_latest_checkpoint(checkpoint)\n",
    "\n",
    "# Create a PlotSamplesCallback instance\n",
    "plot_samples_callback = PlotSamplesCallback(wgan, num_samples=1)\n",
    "\n",
    "class CustomModelCheckpoint(keras.callbacks.Callback):\n",
    "\n",
    "    def clear_training_molecules_folder(self):\n",
    "        folder_path = \"training_molecules\"\n",
    "        if os.path.exists(folder_path):\n",
    "            shutil.rmtree(folder_path)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Update the checkpoint's epoch\n",
    "        checkpoint.epoch.assign(epoch + 1)\n",
    "        # Save the checkpoint at the end of each epoch\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "        # Clear the contents of the training_molecules folder\n",
    "        #self.clear_training_molecules_folder()\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "checkpoint_callback = CustomModelCheckpoint()\n",
    "\n",
    "# Instantiate the GANLogger\n",
    "gan_logger = GANLogger(\n",
    "    tensorboard_logdir, \n",
    "    data['smiles'].tolist(), \n",
    "    full_quinolines_smiles, \n",
    "    num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "#tf.config.run_functions_eagerly(False)\n",
    "print(\"Is TensorFlow running in eager mode:\", tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetMetricsCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Resetting metric states at the end of each epoch\n",
    "        self.model.metric_loss_gp.reset_states()\n",
    "        self.model.metric_discriminator_loss.reset_states()\n",
    "        self.model.metric_disc_loss.reset_states()\n",
    "        self.model.metric_rewards.reset_states()\n",
    "        self.model.metric_connectivity_g.reset_states()\n",
    "        self.model.metric_gen_loss.reset_states()\n",
    "        self.model.metric_critic_g_fake.reset_states()\n",
    "        self.model.metric_critic_g_fake_loss.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def data_generator(adjacency_tensor, feature_tensor, batch_size):\n",
    "    dataset_size = len(adjacency_tensor)\n",
    "    indices = np.arange(dataset_size)\n",
    "    while True:\n",
    "        # Shuffle indices at the start of each epoch\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(0, dataset_size, batch_size):\n",
    "            batch_indices = indices[i: min(i + batch_size, dataset_size)]\n",
    "            batch_adjacency_tensor = adjacency_tensor[batch_indices]\n",
    "            batch_feature_tensor = feature_tensor[batch_indices]\n",
    "            yield [batch_adjacency_tensor, batch_feature_tensor]'''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def data_generator(adjacency_tensor, feature_tensor, batch_size):\n",
    "    dataset_size = len(adjacency_tensor)\n",
    "    indices = np.arange(dataset_size)\n",
    "    while True:\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(0, dataset_size, batch_size):\n",
    "            batch_indices = indices[i: min(i + batch_size, dataset_size)]\n",
    "            batch_adjacency_tensor = tf.convert_to_tensor(adjacency_tensor[batch_indices], dtype=tf.float32)\n",
    "            batch_feature_tensor = tf.convert_to_tensor(feature_tensor[batch_indices], dtype=tf.float32)\n",
    "            yield [batch_adjacency_tensor, batch_feature_tensor]\n",
    "\n",
    "batch_size = 8\n",
    "data_gen = data_generator(adjacency_tensor, feature_tensor, batch_size)\n",
    "\n",
    "\n",
    "batch_size = 8  # Set your batch size, the recommended is 512 but 8 was used for testing\n",
    "data_gen = data_generator(adjacency_tensor, feature_tensor, batch_size)\n",
    "\n",
    "steps_per_epoch = len(adjacency_tensor) // batch_size\n",
    "if len(adjacency_tensor) % batch_size != 0:\n",
    "    steps_per_epoch += 1\n",
    "\n",
    "# Train the model\n",
    "wgan.fit(\n",
    "    data_gen,\n",
    "    initial_epoch=starting_epoch,\n",
    "    epochs= 401,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    callbacks=[\n",
    "        checkpoint_callback, \n",
    "        plot_samples_callback, \n",
    "        gan_logger,\n",
    "        ResetMetricsCallback()\n",
    "        ],\n",
    ")\n",
    "# Save the trained model\n",
    "wgan.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
